{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import data_config as dc\n",
    "wormData = dc.kato.data()\n",
    "len(wormData.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# I hate the list comprehensions too. When we start doing serious analysis this wont happen\n",
    "nname_to_index = [{wormData[i]['NeuronIds'][0][j]:j\n",
    "                   for j in range(len(wormData[i]['NeuronIds'][0])) \n",
    "                   if wormData[i]['NeuronIds'][0][j]!=None}\n",
    "                  for i in range(5)]\n",
    "\n",
    "neurons = [ set(filter(lambda x: x!=None, wormData[i]['NeuronIds'][0].values)) for i in range(5) ]\n",
    "all_shared_neurons = set.intersection(*neurons)\n",
    "\n",
    "# Let's compute shared neurons in the dataset\n",
    "from itertools import combinations\n",
    "groupings  = [ i for i in combinations(range(5), r=2)]\n",
    "similars = {}\n",
    "for i,j in groupings: \n",
    "    ns_i = neurons[i]\n",
    "    ns_j = neurons[j] \n",
    "    similars[(i,j)] = similars[(j,i)] = set.intersection(set(ns_i), set(ns_j))\n",
    "\n",
    "for i in range(5):\n",
    "    groupings.append((i,i))\n",
    "    similars[(i,i)] = set.intersection(set(neurons[i]), set(neurons[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "Now let's build a data structure so we can compare across datasets: we'll use Kato's first assumed neuron names and only later will handle the nuanced case of working with both hypotheses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def crosscorrelate(a,b):\n",
    "  return np.dot(a, b.T)\n",
    "\n",
    "def assemble_indexes_for(i, neurons):\n",
    "    indexes_a = [v for k,v in nname_to_index[i].iteritems() if k in neurons]\n",
    "    return indexes_a\n",
    "\n",
    "def global_similars(i,j):\n",
    "    \"\"\"\n",
    "    Returns a two-tuple containing the indexes in datasets i and j \n",
    "    respectively of the neurons shared amongst the entire dataset. \n",
    "    \"\"\"\n",
    "    \n",
    "    indexes_a = assemble_indexes_for(i, all_shared_neurons)\n",
    "    indexes_b = assemble_indexes_for(j,all_shared_neurons)\n",
    "    return sorted(indexes_a), sorted(indexes_b)\n",
    "\n",
    "\n",
    "def local_similars(i,j):\n",
    "    \"\"\"\n",
    "    Returns a two-tuple containing the indexes in datasets i and j \n",
    "    respectively of the neurons shared amongst datasets i and j. \n",
    "    \"\"\"\n",
    "    neurons = similars[(i,j)]\n",
    "    indexes_a = assemble_indexes_for(i, neurons)\n",
    "    indexes_b = assemble_indexes_for(j,neurons)\n",
    "    return sorted(indexes_a), sorted(indexes_b)\n",
    "\n",
    "def assemble_shared(i,j, dataset_name, indi, indj):\n",
    "    \"\"\"\n",
    "    Returns a two-tuple containing time series for the neurons at the indexes\n",
    "    indi and indj in the i and jth dataset respectively. \n",
    "    \"\"\"\n",
    "    matrix_i = np.take(wormData[i][dataset_name], indi,axis=0)\n",
    "    matrix_j = np.take(wormData[j][dataset_name], indj,axis=0)\n",
    "    return matrix_i, matrix_j\n",
    "\n",
    "def prune(a,b):\n",
    "    \"\"\"\n",
    "    Prunes off indexes from the larger dataset so they have equal dimensions. \n",
    "    Only works for 1D arrays for now. \n",
    "    \"\"\"\n",
    "    last = min(a.shape[1], b.shape[1])\n",
    "    # YES - the is no interpolation for wholly unknown data\n",
    "    # Interpolation iff Parameterized model, both of which are false\n",
    "    # http://stats.stackexchange.com/questions/3589/correlation-between-two-variables-of-unequal-size\n",
    "    return (a[:,0:last], b[:,0:last])\n",
    "    \n",
    "def correlator_data_gen(i,j, dataset_name, fetch_shared_ns):\n",
    "    # We need to prune off the end of the larger dataset so they \n",
    "    # both are of the same length to make cross correlation possible\n",
    "    matri, matrj = prune(\n",
    "        # Assemble the datasets for neurons shared \n",
    "        *assemble_shared(i,j, \n",
    "            dataset_name,\n",
    "            # Fetch shared neurons using abstract metric function\n",
    "            *fetch_shared_ns(i,j)\n",
    "        \n",
    "    )) \n",
    "    \n",
    "    return matri, matrj\n",
    "    \n",
    "    \n",
    "def timeseries_on(dataset):\n",
    "    neurons = sorted(all_shared_neurons)\n",
    "    shared = []\n",
    "    for i in range(len(wormData.T)):\n",
    "        neuron_indexes = [nname_to_index[i][k] for k in neurons]\n",
    "        shared.append(np.take(wormData[i][dataset], neuron_indexes,axis=0))\n",
    "    return shared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print similars.keys()\n",
    "a,b = correlator_data_gen(2,3, 'deltaFOverF_deriv',local_similars)\n",
    "\n",
    "print a.shape\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "cc = crosscorrelate(a,b)\n",
    "plt.pcolormesh(cc)   \n",
    "plt.colorbar()\n",
    "plt.title('Cross Correlation of Neurons in dataset A=2 and B=3')\n",
    "plt.ylabel('Neurons in dataset A')\n",
    "plt.xlabel('Neurons in dataset B')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dimensions_kit as dk\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "dimensions=(4,4)\n",
    "def plot_crosscorrelations(title, dataset, similarity_preference):\n",
    "    print len(groupings)\n",
    "    f, axes = plt.subplots(4,4,figsize=(20,20))\n",
    "    f.suptitle(title, fontsize=20)\n",
    "    n=0\n",
    "    for k,i in groupings:\n",
    "\n",
    "        x,y = dk.transform((4,4),n)\n",
    "\n",
    "        axis = axes[x][y]\n",
    "        a,b = correlator_data_gen(k,i,dataset,similarity_preference)\n",
    "        pc = axis.pcolormesh(crosscorrelate(a,b))\n",
    "        \n",
    "        \n",
    "        div = make_axes_locatable(axis)\n",
    "        cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        cbar = plt.colorbar(pc, cax=cax)\n",
    "        \n",
    "        axis.set_title('Dataset ${0}$ vs ${1}$ Crosscorrelation'.format(k,i))\n",
    "        n+=1\n",
    "\n",
    "    while n < dimensions[0]*dimensions[1]:\n",
    "        x,y = dk.transform(dimensions,n)\n",
    "        axes[x][y].axis('off')\n",
    "        n+=1\n",
    "\n",
    "plot_crosscorrelations('CrossCorr across globally shared neurons', \n",
    "    'deltaFOverF_deriv', local_similars)\n",
    "\n",
    "plot_crosscorrelations('CrossCorr across globally shared neurons', \n",
    "    'deltaFOverF_deriv', global_similars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we are going to plot deltaFOVerF_deriv series over \n",
    "shared neurons across all five datasets\n",
    "\"\"\"\n",
    "import dimensions_kit as dk\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "dimensions=(3,2)\n",
    "start = timer()\n",
    "data = timeseries_on('deltaFOverF_deriv')\n",
    "end = timer()\n",
    "print(end - start) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_shared_datasets(dataset):\n",
    "    data = timeseries_on(dataset)\n",
    "\n",
    "    f, axes = plt.subplots(*dimensions,figsize=(20,20))\n",
    "    for n in range(5):\n",
    "\n",
    "        x,y = dk.transform(dimensions,n)\n",
    "\n",
    "        axis = axes[x][y]\n",
    "        \n",
    "        plotting = data[n]\n",
    "        \n",
    "        pc = axis.pcolormesh(plotting)\n",
    "        \n",
    "        # Dealing with the colorbar\n",
    "        div = make_axes_locatable(axis)\n",
    "        cax = div.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        cbar = plt.colorbar(pc, cax=cax)\n",
    "        \n",
    "        # Setting yticks for easy identification\n",
    "        yticks = sorted(all_shared_neurons)\n",
    "        axis.set_yticks(np.arange(len(yticks))+0.5, minor=False)\n",
    "        axis.set_yticklabels(yticks, minor=False)\n",
    "        \n",
    "        # Setting xticks\n",
    "        xticks = wormData[n]['tv'][0]\n",
    "        axis.set_xticks(np.arange(len(xticks)))\n",
    "        axis.set_xticklabels(xticks)\n",
    "        \n",
    "        axis.set_title('Dataset ${0}$ {1}'.format(n,dataset))\n",
    "        axis.set_ylabel('Neuron')\n",
    "        axis.set_xlabel('Time')\n",
    "\n",
    "    axes[2,1].axis('off')\n",
    "    \n",
    "ns = sorted(all_shared_neurons)\n",
    "print ns\n",
    "print len(ns)\n",
    "print wormData[3]['tv'][0].shape\n",
    "plot_shared_datasets('deltaFOverF_deriv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4/8/16 We've just succeeded in plotting timeseries for all globally shared neurons across all deltaFOverF_derivs. This was a few hours work, just to extract, sort and plot the data. Took quite a bit of fiddling to make the yticks work, but the end result was well worth the effort. Nevertheless, we still need to get rid of that ugly whitespace. I'm not sure how that can be accomplished. \n",
    "\n",
    "The next step, even before implementing FCA, is using sklearn to cluster our time series. I did a little bit of reading and discovered the affinity-propagation algorithm. It turns out sklearn has a good implementation of it. Let me note here two things: \n",
    "\n",
    "1. How important being aware of which data-analysis techniques exist\n",
    "2. Developing the expertise to really understand them, to understand how they perform on different types of datasets, what they unique bring to the table, etc. \n",
    "The alternative is markov clustering, which I'll need to do a little more reading about. When I get to it, here is the wikipedia link: https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo. And here's the link for affinity-propogation (it's a message passing algorithm, https://en.wikipedia.org/wiki/Affinity_propagation)\n",
    "\n",
    "So – let's start with copying over the code from sklearn and then maybe moving on to more sophistocated handwritten analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross correlations are essentially linear convolutions - i.e. linear combinations of two functions, though here we didn't do a delay. \n",
    "\n",
    "\n",
    "### Clustering\n",
    "*do we need to subtract time derivatives from the data to do a \"sliding window\" cross correlation?*\n",
    "If you look at the images above, you'll notice even though cross correlation should describe causal relationships between signals, the causal relationships are fairly inconsistent amond the datasets visually, though there are some boxlike regions. **Once you cluster, things might become clearer.** \n",
    "\n",
    "Covariance and Cross Correlation are both linear combinations of two signals. Just like with PCA, a lot of the nonlinear information is lost (**would be interesting to quantify precisely how much information is lost**). So after doing some research, I discovered this: https://www.researchgate.net/post/how_can_I_find_the_cross-correlation_between_two_time_series_atmospheric_data – we should look for [partial] mututal information. \n",
    "\n",
    "### Taking Advantage of the Datasets\n",
    "Though I haven't studied fourier transforms in depth, it'd be interesting to take a quantitative look on how information travels through neural networks by jumping onto a forward signal and then being taken off the signal by some downstream neuron: here is a good tutorial on FTs: http://www.di.fc.ul.pt/~jpn/r/fourier/fourier.html. \n",
    "\n",
    "### Extending this\n",
    "After running our cross correlation, there are a few ideas that come to mind: \n",
    "\n",
    "- General clustering of neurons in all these measures: cross correlation, PCA, etc. Clustering for different properties\n",
    "- Functional clustering algorithm\n",
    "- **Developing new algorithm to take connectivity into account**\n",
    "- Developing new metrics for more detailed information flow/computational functions of neurons and starting out with plotting them\n",
    "- Running monte carlo simulations on the different computational building blocks – (this is an idea I've had for a while) and then look for these dynamic patterns in our dynamic data. \n",
    "- Monte Carlo neuron annealing based on some heuristic and predicting what will happen with bayesian inference. \n",
    "- Using Machine learning to learn nonlinear parameter metrics and which directions go downwards to parameterize C Elegans. \n",
    "- Particle swarm optimization: https://en.wikipedia.org/wiki/Particle_swarm_optimization\n",
    "- Percolation theory\n",
    "\n",
    "There are many different perspectives on the data – the important thing is getting true insight and not running different procedures for their own sake. \n",
    "\n",
    "One interesting algorithm, which motivated this notebook in the first place, was Sarah Feldt's FCA paper. It is a different perspective on the same data that Kato used. The difference here is we aren't looking at global patterns, but at information flow through functional structure. Nevertheless, what scares me is all these techniques use linear metrics yet the data is **highly nonlinear**. The linear metrics do give a good linear approximation and I'm not sure what the error would be if the metrics weren't linear, but I'm wondering how we can become more exact. \n",
    "\n",
    "### Fourier Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to feel like I've really gotten into numerical analysis, I'm going to run a standard FFT on my time series. I'm doing this without deeply understanding the algorithm because *first experiment* then *understand*. Or after the biblical maxim *aharei hapiulot nimshehet halevavot*.\n",
    "\n",
    "Here's what numpy says about fourier analysis: \n",
    "\n",
    "> Fourier analysis is fundamentally a method for expressing a function as a sum of periodic components, and for recovering the function from those components. When both the function and its Fourier transform are replaced with discretized counterparts, it is called the discrete Fourier transform (DFT). The DFT has become a mainstay of numerical computing in part because of a very fast algorithm for computing it, called the Fast Fourier Transform (FFT), which was known to Gauss (1805) and was brought to light in its current form by Cooley and Tukey [CT]. Press et al. [NR] provide an accessible introduction to Fourier analysis and its applications.\n",
    "\n",
    "We are really doing to use the discrete version (DFT) using *difference equations* somewhre (thanks Knuth and Concrete Mathematics) but the ideas are the same.\n",
    "\n",
    "$A_k=\\sum_{m=0}^{n-1}a_m\\exp\\begin{Bmatrix}-2\\pi i\\frac{mk}{n}\\end{Bmatrix}\\ k=0,...,n-1$\n",
    "\n",
    "If you'll notice this is really the sover every point. The discrete version would be an integral. \n",
    "\n",
    "If you've encountered Euler's formula before you'll understand this equation. We know from Euler (and the proof is with Taylor expansions of $sin(x)$ and $cos(x)$ that $e^{ix}=cos(x)+isin(x)$. Without full expertise in the math-theoretic details, I know the left side of the equation yields oscillatory behavior from the special case of Euler's formula $e^{i\\pi}=-1$. Taking the square root of both sides yields i: $e^{i\\frac{\\pi}{2}}=i$, which we know when raised to the $x^{th}$ power os oscillatory. Hence $\\exp\\begin{Bmatrix}^{ix\\frac{\\pi}{2}}\\end{Bmatrix}$ is oscillatory (thanks to Dov Greenwood for explaining to me this morning what Taylor series are and how they can be used to prove Euler's formula; the brilliance isn't the proof at all. It's the mind which made the observation)\n",
    "\n",
    "Adding up the different waves is the basis of fourier transforms, a really beautiful topic in pure and numerical maths. \n",
    "\n",
    "So let's start fouriering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lets do this ft thinga majing\n",
    "import scipy.fftpack\n",
    "\n",
    "timeseries = wormData[0]['deltaFOverF'][34]\n",
    "x = wormData[0]['tv'][0]\n",
    "\n",
    "N=x.shape[0]\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title('DeltaFOverF for a random neuron')\n",
    "plt.ylabel('Ca+ Fluorescence')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.plot(x,timeseries)\n",
    "\n",
    "\n",
    "w = scipy.fftpack.rfft(timeseries)\n",
    "f = scipy.fftpack.rfftfreq(N, x[1]-x[0])\n",
    "spectrum = w**2\n",
    "\n",
    "y2 = scipy.fftpack.irfft(w)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Adding fourier waves')\n",
    "plt.ylim((-1.5,1.5))\n",
    "plt.plot(x,w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was evident, I do not fully understand Fouriers, but now have a better untuition for what they do. So I'll need to go read about fouriers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
